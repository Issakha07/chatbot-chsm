"""
Script de r√©indexation automatique des documents
D√©tecte les nouveaux PDFs et met √† jour ChromaDB
"""

import os
import sys
from pathlib import Path
import chromadb
from sentence_transformers import SentenceTransformer
import logging
import hashlib
import json
from datetime import datetime

# Ajouter le chemin parent pour imports
sys.path.insert(0, str(Path(__file__).parent.parent))
from backend.document_processor import DocumentProcessor

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

class DocumentIndexer:
    """Gestion de l'indexation des documents"""
    
    def __init__(self, documents_dir: str = "./documents"):
        self.documents_dir = Path(documents_dir)
        self.metadata_file = Path(".dvc/document_metadata.json")
        self.doc_processor = DocumentProcessor(documents_dir)
        
        # ChromaDB
        self.chroma_client = chromadb.Client()
        try:
            self.collection = self.chroma_client.get_collection("documents")
            logger.info("‚úÖ Collection ChromaDB charg√©e")
        except:
            self.collection = self.chroma_client.create_collection("documents")
            logger.info("‚úÖ Nouvelle collection ChromaDB cr√©√©e")
        
        # Mod√®le d'embeddings
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def get_file_hash(self, file_path: Path) -> str:
        """Calculer le hash MD5 d'un fichier"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def load_metadata(self) -> dict:
        """Charger les m√©tadonn√©es des documents index√©s"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def save_metadata(self, metadata: dict):
        """Sauvegarder les m√©tadonn√©es"""
        self.metadata_file.parent.mkdir(exist_ok=True)
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def detect_changes(self) -> dict:
        """D√©tecter les documents nouveaux/modifi√©s/supprim√©s"""
        metadata = self.load_metadata()
        current_files = {f.name: self.get_file_hash(f) 
                        for f in self.documents_dir.glob("*.pdf")}
        
        changes = {
            "new": [],
            "modified": [],
            "deleted": [],
            "unchanged": []
        }
        
        # Nouveaux ou modifi√©s
        for filename, file_hash in current_files.items():
            if filename not in metadata:
                changes["new"].append(filename)
            elif metadata[filename]["hash"] != file_hash:
                changes["modified"].append(filename)
            else:
                changes["unchanged"].append(filename)
        
        # Supprim√©s
        for filename in metadata.keys():
            if filename not in current_files:
                changes["deleted"].append(filename)
        
        return changes, current_files
    
    def chunk_text(self, text: str, chunk_size: int = 1000) -> list:
        """D√©couper le texte en chunks"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            current_length += len(word) + 1
            if current_length > chunk_size:
                chunks.append(" ".join(current_chunk))
                current_chunk = [word]
                current_length = len(word)
            else:
                current_chunk.append(word)
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks
    
    def remove_document_chunks(self, filename: str):
        """Supprimer tous les chunks d'un document de ChromaDB"""
        try:
            # R√©cup√©rer tous les IDs avec ce filename
            results = self.collection.get(
                where={"source": filename}
            )
            
            if results['ids']:
                self.collection.delete(ids=results['ids'])
                logger.info(f"üóëÔ∏è  Supprim√© {len(results['ids'])} chunks de {filename}")
        except Exception as e:
            logger.error(f"Erreur suppression {filename}: {e}")
    
    def index_document(self, filename: str):
        """Indexer un document dans ChromaDB"""
        file_path = self.documents_dir / filename
        
        # Extraire le texte
        text = self.doc_processor.extract_text_from_pdf(file_path)
        if not text:
            logger.warning(f"‚ö†Ô∏è  Pas de texte extrait de {filename}")
            return 0
        
        # D√©couper en chunks
        chunks = self.chunk_text(text)
        logger.info(f"üìÑ {filename}: {len(chunks)} chunks cr√©√©s")
        
        # G√©n√©rer embeddings
        embeddings = self.embedding_model.encode(chunks).tolist()
        
        # Pr√©parer les donn√©es
        chunk_ids = [f"{filename}_chunk_{i}" for i in range(len(chunks))]
        metadatas = [{"source": filename} for _ in chunks]
        
        # Ajouter √† ChromaDB
        self.collection.add(
            embeddings=embeddings,
            documents=chunks,
            metadatas=metadatas,
            ids=chunk_ids
        )
        
        return len(chunks)
    
    def reindex_all(self):
        """R√©indexer tous les documents"""
        logger.info("üîÑ R√©indexation compl√®te de tous les documents...")
        
        # Supprimer toute la collection
        self.chroma_client.delete_collection("documents")
        self.collection = self.chroma_client.create_collection("documents")
        logger.info("üóëÔ∏è  Collection ChromaDB r√©initialis√©e")
        
        # Indexer tous les PDFs
        pdf_files = list(self.documents_dir.glob("*.pdf"))
        total_chunks = 0
        metadata = {}
        
        for pdf_file in pdf_files:
            chunks = self.index_document(pdf_file.name)
            total_chunks += chunks
            
            metadata[pdf_file.name] = {
                "hash": self.get_file_hash(pdf_file),
                "chunks": chunks,
                "indexed_at": datetime.now().isoformat()
            }
        
        self.save_metadata(metadata)
        logger.info(f"‚úÖ R√©indexation termin√©e: {len(pdf_files)} documents, {total_chunks} chunks")
    
    def incremental_reindex(self):
        """R√©indexation incr√©mentale (seulement les changements)"""
        logger.info("üîç D√©tection des changements...")
        
        changes, current_files = self.detect_changes()
        
        # Afficher le r√©sum√©
        logger.info(f"""
üìä Changements d√©tect√©s:
   - Nouveaux: {len(changes['new'])}
   - Modifi√©s: {len(changes['modified'])}
   - Supprim√©s: {len(changes['deleted'])}
   - Inchang√©s: {len(changes['unchanged'])}
        """)
        
        if not any([changes['new'], changes['modified'], changes['deleted']]):
            logger.info("‚úÖ Aucun changement d√©tect√©")
            return
        
        metadata = self.load_metadata()
        
        # Traiter les suppressions
        for filename in changes['deleted']:
            self.remove_document_chunks(filename)
            del metadata[filename]
        
        # Traiter les modifications (supprimer puis r√©indexer)
        for filename in changes['modified']:
            logger.info(f"üîÑ Mise √† jour: {filename}")
            self.remove_document_chunks(filename)
            chunks = self.index_document(filename)
            metadata[filename] = {
                "hash": current_files[filename],
                "chunks": chunks,
                "indexed_at": datetime.now().isoformat()
            }
        
        # Traiter les nouveaux documents
        for filename in changes['new']:
            logger.info(f"‚ûï Nouveau document: {filename}")
            chunks = self.index_document(filename)
            metadata[filename] = {
                "hash": current_files[filename],
                "chunks": chunks,
                "indexed_at": datetime.now().isoformat()
            }
        
        self.save_metadata(metadata)
        logger.info("‚úÖ R√©indexation incr√©mentale termin√©e")


def main():
    """Point d'entr√©e du script"""
    import argparse
    
    parser = argparse.ArgumentParser(description="R√©indexation des documents")
    parser.add_argument(
        "--mode",
        choices=["full", "incremental"],
        default="incremental",
        help="Mode de r√©indexation (full=tout, incremental=changements)"
    )
    parser.add_argument(
        "--documents-dir",
        default="./documents",
        help="Chemin vers le dossier documents"
    )
    
    args = parser.parse_args()
    
    indexer = DocumentIndexer(args.documents_dir)
    
    if args.mode == "full":
        indexer.reindex_all()
    else:
        indexer.incremental_reindex()


if __name__ == "__main__":
    main()
